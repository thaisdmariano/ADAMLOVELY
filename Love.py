#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import random
import re as _re
from typing import List, Dict, Tuple, Set, Any, Optional

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset

import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURAÃ‡ÃƒO DE ARQUIVOS E CONSTANTES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ARQUIVO_MEMORIA = "Adam_Lovely_memory.json"
ARQUIVO_INCONSCIENTE = "Adam_Lovely_inconscious.json"
EMBED_DIM = 16
HIDDEN_DIM = 64
PATIENCE = 5
BATCH_SIZE = 8
LR = 1e-3
EPOCHS = 50
UNK = "<UNK>"
UNK_VAL = -1.0
N_GRAM = 2  # Tamanho do n-grama (2 para bigrams)


## INSEPA_TOKENIZER
def generate_ngrams(token: str, n: int) -> List[str]:
    """Gera n-gramas de caracteres de um token."""
    if len(token) < n:
        return [token]  # Se menor que n, retorna o token inteiro
    return [token[i:i + n] for i in range(len(token) - n + 1)]


def ckpt_path(dominio: str) -> str:
    return f"insepa_{dominio}.pt"


def Token(text: str) -> List[str]:
    """INSEPA tokenizaÃ§Ã£o: mantÃ©m palavras, pontuaÃ§Ã£o, emojis, stopwords."""
    return _re.findall(r'\w+|[^\w\s]', text, _re.UNICODE)


def next_marker(prev: str) -> str:
    """Incrementa sem arredondar: 0.99 â†’ 0.100"""
    mom, _, suf = prev.partition('.')
    if not mom.isdigit():
        raise ValueError(f"Marcador invÃ¡lido: {prev!r}")
    return f"{mom}.{int(suf or '0') + 1}"


def generate_markers(start: str, count: int) -> List[str]:
    seq, cur = [], start
    for _ in range(count):
        cur = next_marker(cur)
        seq.append(cur)
    return seq


## INSEPA_UTILS
def carregar_json(caminho: str, default: dict) -> dict:
    if not os.path.exists(caminho):
        with open(caminho, "w", encoding="utf-8") as f:
            json.dump(default, f, ensure_ascii=False, indent=2)
        return default
    with open(caminho, "r", encoding="utf-8") as f:
        return json.load(f)


def salvar_json(caminho: str, data: dict) -> None:
    with open(caminho, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def garantir_pontuacao(txt: str) -> str:
    txt = txt.strip()
    return txt if txt and txt[-1] in ".!?" else (txt + "." if txt else "")


def normalize_collapse_spaces(txt: str) -> str:
    return _re.sub(r'\s+', ' ', txt).strip()


def normalize_separators(txt: str) -> str:
    return _re.sub(r'\s*([,.;:])\s*', '', txt).strip()


def normalize(txt: str) -> str:
    for fn in (normalize_collapse_spaces, normalize_separators):
        txt = fn(txt)
    return txt


def get_variations_for_tokens(im_id: str, bloco_id: int, campo: str, markers: List[str]) -> List[str]:
    """ObtÃ©m variaÃ§Ãµes de tokens para marcadores especÃ­ficos."""
    inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
    bloco_inco = next((b for b in inconsciente["INCO"][im_id]["Blocos"] if b["Bloco_id"] == str(bloco_id)), None)
    if bloco_inco:
        variations = set()
        for marker in markers:
            if marker in bloco_inco[campo]:
                data = bloco_inco[campo][marker]
                variations.add(normalize(data["token"]))
                for var in data.get("vars", []):
                    variations.add(normalize(var))
        return list(variations)
    return []


def parse_text_reaction(prompt: str, reactions: Set[str]) -> Tuple[str, str]:
    s = prompt.strip()
    sorted_reactions = sorted(reactions, key=len, reverse=True)
    for reac in sorted_reactions:
        if reac and s.endswith(reac):
            txt = s[:-len(reac)].rstrip()
            return txt, reac
    return s, ""


def build_field_vocabs(memoria: dict, dominio: str) -> Dict[str, Dict[str, int]]:
    blocos = memoria["IM"][dominio]["blocos"]
    sets = {"E": set(), "RE": set(), "CE": set(), "PIDE": set()}
    for b in blocos:
        t = b["entrada"]["tokens"]
        for f in sets:
            for tok in t.get(f, []):
                ngrams = generate_ngrams(tok, N_GRAM)
                sets[f].update(ngrams)
    return {
        f: {ng: i + 1 for i, ng in enumerate(sorted(sets[f]))}
        for f in sets
    }


def build_label_vocabs(memoria: dict, dominio: str) -> Dict[str, Dict[str, int]]:
    blocos = memoria["IM"][dominio]["blocos"]
    sets = {"texto": set(), "emoji": set(), "ctx": set()}
    for b in blocos:
        for s in b.get("saidas", []):
            for v in s.get("textos", []):
                sets["texto"].add(normalize(v))
            emo = s.get("reacao", "")
            if emo:   sets["emoji"].add(emo)
            ctx = s.get("contexto", "")
            if ctx:   sets["ctx"].add(normalize(ctx))
    return {
        f: {tok: i for i, tok in enumerate(sorted(sets[f]))}
        for f in sets
    }


## INSEPA_DATASET
class InsepaFieldDataset(Dataset):
    def __init__(self, memoria: dict, dominio: str):
        blocos = memoria["IM"][dominio]["blocos"]
        inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
        self.ultimo_child_per_block = {}
        if dominio in inconsciente.get("INCO", {}):
            blocos_inco = inconsciente["INCO"][dominio].get("Blocos", [])
            for bloco in blocos_inco:
                bloco_num = int(bloco["Bloco_id"])
                saida_vals = [float(key) for key in bloco.get("SAÃDA", {}).keys()]
                if saida_vals:
                    self.ultimo_child_per_block[bloco_num] = max(saida_vals)
                else:
                    self.ultimo_child_per_block[bloco_num] = 0.50

        # Coletar tokens Ãºnicos por campo diretamente do JSON
        sets = {"E": set(), "RE": set(), "CE": set(), "PIDE": set()}
        for b in blocos:
            for f in sets:
                sets[f] |= set(b["entrada"]["tokens"].get(f, []))

        self.v_E = {tok: i + 1 for i, tok in enumerate(sorted(sets["E"]))}
        self.v_RE = {tok: i + 1 for i, tok in enumerate(sorted(sets["RE"]))}
        self.v_CE = {tok: i + 1 for i, tok in enumerate(sorted(sets["CE"]))}
        self.v_PIDE = {tok: i + 1 for i, tok in enumerate(sorted(sets["PIDE"]))}
        self.v_E[UNK] = len(self.v_E)
        self.v_RE[UNK] = len(self.v_RE)
        self.v_CE[UNK] = len(self.v_CE)
        self.v_PIDE[UNK] = len(self.v_PIDE)

        # val_to_idx por campo: tokens Ãºnicos como chaves
        self.val_to_idx_E = {tok: i for i, tok in enumerate(sorted(sets["E"]))}
        self.val_to_idx_RE = {tok: i for i, tok in enumerate(sorted(sets["RE"]))}
        self.val_to_idx_CE = {tok: i for i, tok in enumerate(sorted(sets["CE"]))}
        self.val_to_idx_PIDE = {tok: i for i, tok in enumerate(sorted(sets["PIDE"]))}

        self.max_E = max(len(b["entrada"]["tokens"].get("E", [])) for b in blocos)
        self.max_RE = max(len(b["entrada"]["tokens"].get("RE", [])) for b in blocos)
        self.max_CE = max(len(b["entrada"]["tokens"].get("CE", [])) for b in blocos)
        self.max_PIDE = max(len(b["entrada"]["tokens"].get("PIDE", [])) for b in blocos)
        self.max_pos = max(self.max_E, self.max_RE, self.max_CE, self.max_PIDE)

        # Calcular max n-gramas por token
        all_tokens = set()
        for b in blocos:
            for field in ["E", "RE", "CE", "PIDE"]:
                all_tokens |= set(b["entrada"]["tokens"].get(field, []))
        self.max_ng = max(len(generate_ngrams(t, N_GRAM)) for t in all_tokens if t) if all_tokens else 1
        self.max_E_ng = self.max_E * self.max_ng
        self.max_RE_ng = self.max_RE * self.max_ng
        self.max_CE_ng = self.max_CE * self.max_ng
        self.max_PIDE_ng = self.max_PIDE * self.max_ng

        # calcula mom_size = maior mÃ£e + 1
        max_mom = 0
        for b in blocos:
            for tok in b["entrada"]["tokens"].get("TOTAL", []):
                m = int(tok.split(".", 1)[0])
                if m > max_mom: max_mom = m
        self.mom_size = max_mom + 1

        # valores Ãºnicos para posiÃ§Ãµes fixas (nÃ£o usado agora, mas manter compatibilidade)
        vals = {float(t) for t in all_tokens if t}
        sorted_vals = sorted(vals)
        self.val_to_idx = {v: i + 1 for i, v in enumerate(sorted_vals)}  # Ã­ndices de 1 em diante, 0 para padding
        self.num_vals = len(sorted_vals)

        # vocabulÃ¡rios de rÃ³tulos por bloco
        self.n_txt = max(len(b["saidas"][0]["textos"]) for b in blocos)
        self.n_emo = max(1, len(set(b["saidas"][0].get("reacao", "") for b in blocos if b["saidas"][0].get("reacao"))))
        self.n_ctx = max(1, len(set(
            normalize(b["saidas"][0].get("contexto", "")) for b in blocos if b["saidas"][0].get("contexto"))))

        self.pares: List[Tuple[Dict, Dict]] = []
        for b in blocos:
            bloco_id = b["bloco_id"]
            max_val = self.ultimo_child_per_block.get(bloco_id, 0.50)

            # Usar tokens fixos do JSON
            E_tokens = b["entrada"]["tokens"].get("E", [])
            RE_tokens = b["entrada"]["tokens"].get("RE", [])
            CE_tokens = b["entrada"]["tokens"].get("CE", [])
            PIDE_tokens = b["entrada"]["tokens"].get("PIDE", [])

            # Gerar n-gramas e ids
            E_ngrams = [generate_ngrams(t, N_GRAM) for t in E_tokens]
            RE_ngrams = [generate_ngrams(t, N_GRAM) for t in RE_tokens]
            CE_ngrams = [generate_ngrams(t, N_GRAM) for t in CE_tokens]
            PIDE_ngrams = [generate_ngrams(t, N_GRAM) for t in PIDE_tokens]

            E_ids = [self.v_E.get(ng, self.v_E.get(UNK, 0)) for nglist in E_ngrams for ng in nglist]
            RE_ids = [self.v_RE.get(ng, self.v_RE.get(UNK, 0)) for nglist in RE_ngrams for ng in nglist]
            CE_ids = [self.v_CE.get(ng, self.v_CE.get(UNK, 0)) for nglist in CE_ngrams for ng in nglist]
            PIDE_ids = [self.v_PIDE.get(ng, self.v_PIDE.get(UNK, 0)) for nglist in PIDE_ngrams for ng in nglist]

            E_ids += [0] * (self.max_E_ng - len(E_ids))
            RE_ids += [0] * (self.max_RE_ng - len(RE_ids))
            CE_ids += [0] * (self.max_CE_ng - len(CE_ids))
            PIDE_ids += [0] * (self.max_PIDE_ng - len(PIDE_ids))

            # Ã­ndices de valores para embedding (mantÃ©m tokens)
            E_val_idxs = [self.val_to_idx_E.get(t, 0) for t in E_tokens]
            RE_val_idxs = [self.val_to_idx_RE.get(t, 0) for t in RE_tokens]
            CE_val_idxs = [self.val_to_idx_CE.get(t, 0) for t in CE_tokens]
            PIDE_val_idxs = [self.val_to_idx_PIDE.get(t, 0) for t in PIDE_tokens]
            E_val_idxs += [0] * (self.max_E - len(E_val_idxs))
            RE_val_idxs += [0] * (self.max_RE - len(RE_val_idxs))
            CE_val_idxs += [0] * (self.max_CE - len(CE_val_idxs))
            PIDE_val_idxs += [0] * (self.max_PIDE - len(PIDE_val_idxs))

            # funÃ§Ã£o para gerar valores, mÃ£es e posiÃ§Ãµes
            def build_feats(tokens, maxlen):
                vals = [float(tok) for tok in tokens]
                moms = [int(tok.split(".", 1)[0]) for tok in tokens]
                if vals:
                    min_v, max_v = min(vals), max(vals)
                    pos = [(v - min_v) / (max_v - min_v) if max_v > min_v else 0.0 for v in vals]
                else:
                    pos = []
                pad = maxlen - len(tokens)
                vals += [0.0] * pad
                moms += [0] * pad
                pos += [0.0] * pad
                return vals, moms, pos

            E_vals, E_moms, E_pos = build_feats(E_tokens, self.max_E)
            RE_vals, RE_moms, RE_pos = build_feats(RE_tokens, self.max_RE)
            CE_vals, CE_moms, CE_pos = build_feats(CE_tokens, self.max_CE)
            PI_vals, PI_moms, PI_pos = build_feats(PIDE_tokens, self.max_PIDE)

            for s in b.get("saidas", []):
                # calcula pos_label = mÃ©dia dos valores dos tokens no bloco
                all_vals = []
                for tokens in [E_tokens, RE_tokens, CE_tokens, PIDE_tokens]:
                    all_vals.extend([float(t) for t in tokens])
                pos_label = sum(all_vals) / len(all_vals) if all_vals else 0.0

                y = {
                    "texto": b["saidas"][0]["textos"].index(s["textos"][0]),
                    "emoji": 0 if b["saidas"][0].get("reacao", "") == s.get("reacao", "") else 1,  # simplificar
                    "ctx": 0 if normalize(b["saidas"][0].get("contexto", "")) == normalize(
                        s.get("contexto", "")) else 1,
                    "pos": pos_label,
                }
                x = {
                    "E": E_ids, "E_val": E_vals, "E_mom": E_moms, "E_pos": E_pos, "E_val_idx": E_val_idxs,
                    "RE": RE_ids, "RE_val": RE_vals, "RE_mom": RE_moms, "RE_pos": RE_pos, "RE_val_idx": RE_val_idxs,
                    "CE": CE_ids, "CE_val": CE_vals, "CE_mom": CE_moms, "CE_pos": CE_pos, "CE_val_idx": CE_val_idxs,
                    "PIDE": PIDE_ids, "PIDE_val": PI_vals, "PIDE_mom": PI_moms, "PIDE_pos": PI_pos,
                    "PIDE_val_idx": PIDE_val_idxs,
                }
                self.pares.append((x, y))

    def __len__(self) -> int:
        return len(self.pares)

    def __getitem__(self, idx: int):
        x, y = self.pares[idx]
        x_t = {
            "E": torch.tensor(x["E"], dtype=torch.long),
            "E_val": torch.tensor(x["E_val"], dtype=torch.float32),
            "E_mom": torch.tensor(x["E_mom"], dtype=torch.long),
            "E_pos": torch.tensor(x["E_pos"], dtype=torch.long),
            "E_val_idx": torch.tensor(x["E_val_idx"], dtype=torch.long),

            "RE": torch.tensor(x["RE"], dtype=torch.long),
            "RE_val": torch.tensor(x["RE_val"], dtype=torch.float32),
            "RE_mom": torch.tensor(x["RE_mom"], dtype=torch.long),
            "RE_pos": torch.tensor(x["RE_pos"], dtype=torch.long),
            "RE_val_idx": torch.tensor(x["RE_val_idx"], dtype=torch.long),

            "CE": torch.tensor(x["CE"], dtype=torch.long),
            "CE_val": torch.tensor(x["CE_val"], dtype=torch.float32),
            "CE_mom": torch.tensor(x["CE_mom"], dtype=torch.long),
            "CE_pos": torch.tensor(x["CE_pos"], dtype=torch.long),
            "CE_val_idx": torch.tensor(x["CE_val_idx"], dtype=torch.long),

            "PIDE": torch.tensor(x["PIDE"], dtype=torch.long),
            "PIDE_val": torch.tensor(x["PIDE_val"], dtype=torch.float32),
            "PIDE_mom": torch.tensor(x["PIDE_mom"], dtype=torch.long),
            "PIDE_pos": torch.tensor(x["PIDE_pos"], dtype=torch.long),
            "PIDE_val_idx": torch.tensor(x["PIDE_val_idx"], dtype=torch.long),
        }
        y_t = {
            "texto": torch.tensor(y["texto"], dtype=torch.long),
            "emoji": torch.tensor(y["emoji"], dtype=torch.long),
            "ctx": torch.tensor(y["ctx"], dtype=torch.long),
            "pos": torch.tensor(y["pos"], dtype=torch.float32),
        }
        return x_t, y_t


## INSEPA_MODEL
class AdamSegmentado(nn.Module):
    def __init__(self,
                 nE: int, nRE: int, nCE: int, nPIDE: int,
                 mom_size: int,
                 num_vals_E: int, num_vals_RE: int, num_vals_CE: int, num_vals_PIDE: int,
                 n_txt: int, n_emo: int, n_ctx: int,
                 max_E: int, max_RE: int, max_CE: int, max_PIDE: int, max_ng: int):
        super().__init__()
        # Embeddings por valor, separados por campo (treinÃ¡veis)
        self.em_Eval = nn.Embedding(num_vals_E, EMBED_DIM)
        self.em_REval = nn.Embedding(num_vals_RE, EMBED_DIM)
        self.em_CEval = nn.Embedding(num_vals_CE, EMBED_DIM)
        self.em_PIDEval = nn.Embedding(num_vals_PIDE, EMBED_DIM)

        # Embeddings para tokens, mÃ£es e projeÃ§Ãµes de posiÃ§Ã£o
        self.em_E = nn.Embedding(nE, EMBED_DIM)
        self.em_RE = nn.Embedding(nRE, EMBED_DIM)
        self.em_CE = nn.Embedding(nCE, EMBED_DIM)
        self.em_PIDE = nn.Embedding(nPIDE, EMBED_DIM)
        self.em_Emom = nn.Embedding(mom_size, EMBED_DIM)
        self.em_REmom = nn.Embedding(mom_size, EMBED_DIM)
        self.em_CEmom = nn.Embedding(mom_size, EMBED_DIM)
        self.em_PIDEmom = nn.Embedding(mom_size, EMBED_DIM)
        self.proj_Epos = nn.Linear(1, EMBED_DIM)
        self.proj_REpos = nn.Linear(1, EMBED_DIM)
        self.proj_CEpos = nn.Linear(1, EMBED_DIM)
        self.proj_PIDEpos = nn.Linear(1, EMBED_DIM)

        self.max_E = max_E
        self.max_RE = max_RE
        self.max_CE = max_CE
        self.max_PIDE = max_PIDE
        self.max_ng = max_ng

        # Transformer Encoder para processar sequÃªncia de campos
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=EMBED_DIM, nhead=4, dim_feedforward=HIDDEN_DIM),
            num_layers=1
        )

        self.fc1 = nn.Linear(EMBED_DIM, HIDDEN_DIM)
        self.act = nn.ReLU()

        # CabeÃ§as de saÃ­da
        self.h_txt = nn.Linear(HIDDEN_DIM, n_txt)
        self.h_emo = nn.Linear(HIDDEN_DIM, n_emo)
        self.h_ctx = nn.Linear(HIDDEN_DIM, n_ctx)
        self.h_pos = nn.Linear(HIDDEN_DIM, 1)  # Nova cabeÃ§a para posiÃ§Ã£o (regressÃ£o)

    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        batch = x["E"].shape[0]
        # Campo E
        eE_tok = self.em_E(x["E"]).view(batch, self.max_E, self.max_ng, EMBED_DIM).mean(dim=2)
        eE_val = self.em_Eval(x["E_val_idx"])
        eE_mom = self.em_Emom(x["E_mom"])
        eE_pos = self.proj_Epos(x["E_val"].unsqueeze(-1))
        eE = (eE_tok + eE_val + eE_mom + eE_pos).mean(dim=1)
        # Campo RE
        eRE_tok = self.em_RE(x["RE"]).view(batch, self.max_RE, self.max_ng, EMBED_DIM).mean(dim=2)
        eRE_val = self.em_REval(x["RE_val_idx"])
        eRE_mom = self.em_REmom(x["RE_mom"])
        eRE_pos = self.proj_REpos(x["RE_val"].unsqueeze(-1))
        eRE = (eRE_tok + eRE_val + eRE_mom + eRE_pos).mean(dim=1)
        # Campo CE
        eCE_tok = self.em_CE(x["CE"]).view(batch, self.max_CE, self.max_ng, EMBED_DIM).mean(dim=2)
        eCE_val = self.em_CEval(x["CE_val_idx"])
        eCE_mom = self.em_CEmom(x["CE_mom"])
        eCE_pos = self.proj_CEpos(x["CE_val"].unsqueeze(-1))
        eCE = (eCE_tok + eCE_val + eCE_mom + eCE_pos).mean(dim=1)
        # Campo PIDE
        ePI_tok = self.em_PIDE(x["PIDE"]).view(batch, self.max_PIDE, self.max_ng, EMBED_DIM).mean(dim=2)
        ePI_val = self.em_PIDEval(x["PIDE_val_idx"])
        ePI_mom = self.em_PIDEmom(x["PIDE_mom"])
        ePI_pos = self.proj_PIDEpos(x["PIDE_val"].unsqueeze(-1))
        ePIDE = (ePI_tok + ePI_val + ePI_mom + ePI_pos).mean(dim=1)

        # Agrega e classifica
        # Empilhar embeddings dos campos em sequÃªncia
        seq = torch.stack([eE, eRE, eCE, ePIDE], dim=1)  # (batch, 4, EMBED_DIM)
        seq = seq.permute(1, 0, 2)  # (4, batch, EMBED_DIM)
        transformed = self.transformer(seq)  # (4, batch, EMBED_DIM)
        transformed = transformed.permute(1, 0, 2)  # (batch, 4, EMBED_DIM)
        h = transformed.mean(dim=1)  # (batch, EMBED_DIM)
        h = self.act(self.fc1(h))
        return {
            "texto": self.h_txt(h),
            "emoji": self.h_emo(h),
            "ctx": self.h_ctx(h),
            "pos": self.h_pos(h),  # PosiÃ§Ã£o como valor numÃ©rico
        }


## INSEPA_TRAIN
def train(memoria: dict, dominio: str) -> None:
    # Atualizar inconsciente para o IM selecionado
    atualizar_inconsciente_para_im(memoria, dominio)

    ds = InsepaFieldDataset(memoria, dominio)
    n = len(ds)
    ckpt = ckpt_path(dominio)

    idxs = list(range(n))
    random.shuffle(idxs)
    vsz = min(max(1, int(0.2 * n)), n - 1)  # garantir pelo menos 1 para treino
    vidx, tidx = idxs[:vsz], idxs[vsz:]
    if not tidx:  # se tidx vazio, usar todos para treino, sem val
        tidx = idxs
        vidx = []
    train_ld = DataLoader(Subset(ds, tidx), batch_size=min(BATCH_SIZE, len(tidx)), shuffle=True)
    val_ld = DataLoader(Subset(ds, vidx), batch_size=min(BATCH_SIZE, len(vidx))) if vidx else None

    model = AdamSegmentado(
        nE=len(ds.v_E), nRE=len(ds.v_RE),
        nCE=len(ds.v_CE), nPIDE=len(ds.v_PIDE),
        mom_size=ds.mom_size,
        num_vals_E=len(ds.val_to_idx_E), num_vals_RE=len(ds.val_to_idx_RE),
        num_vals_CE=len(ds.val_to_idx_CE), num_vals_PIDE=len(ds.val_to_idx_PIDE),
        n_txt=ds.n_txt, n_emo=ds.n_emo,
        n_ctx=ds.n_ctx,
        max_E=ds.max_E, max_RE=ds.max_RE, max_CE=ds.max_CE, max_PIDE=ds.max_PIDE, max_ng=ds.max_ng
    )
    opt = optim.Adam(model.parameters(), lr=LR)
    ce = nn.CrossEntropyLoss()
    mse = nn.MSELoss()

    best, wait, prev_val = float("inf"), 0, None
    progress_bar = st.progress(0)
    status_text = st.empty()
    for ep in range(1, EPOCHS + 1):
        model.train()
        for x, y in train_ld:
            opt.zero_grad()
            out = model(x)
            loss = (
                    ce(out["texto"], y["texto"]) +
                    ce(out["emoji"], y["emoji"]) +
                    ce(out["ctx"], y["ctx"]) +
                    mse(out["pos"], y["pos"])
            )
            loss.backward()
            opt.step()

        model.eval()
        val_loss = 0.0
        if val_ld:
            with torch.no_grad():
                for x, y in val_ld:
                    out = model(x)
                    val_loss += (
                            ce(out["texto"], y["texto"]).item() +
                            ce(out["emoji"], y["emoji"]).item() +
                            ce(out["ctx"], y["ctx"]).item() +
                            mse(out["pos"], y["pos"]).item()
                    )
            val_loss /= len(val_ld)
        else:
            val_loss = float("inf")  # sem validaÃ§Ã£o, usar inf para nÃ£o salvar

        if prev_val is None or val_loss < best:
            best, wait = val_loss, 0
            torch.save((
                model.state_dict(),
                ds.max_E, ds.max_RE, ds.max_CE, ds.max_PIDE,
                ds.mom_size, ds.val_to_idx_E, ds.val_to_idx_RE, ds.val_to_idx_CE, ds.val_to_idx_PIDE,
                ds.v_E, ds.v_RE, ds.v_CE, ds.v_PIDE,
                ds.n_txt, ds.n_emo, ds.n_ctx,
                ds.max_ng
            ), ckpt)
        else:
            wait += 1
            if wait >= PATIENCE:
                break
        prev_val = val_loss
        progress_bar.progress(ep / EPOCHS)
        status_text.text(f"Ã‰poca {ep}/{EPOCHS}, Val Loss: {val_loss:.4f}")

    st.success(f"âœ… Treino concluÃ­do. best_val_loss={best:.4f}")


def infer(memoria: dict, dominio: str) -> None:
    """
    Interface de chat inovadora para inferÃªncia.
    """
    import os, torch, random
    # parse_text_reaction, normalize, ckpt_path, train, AdamSegmentado jÃ¡ disponÃ­veis

    # Atualizar inconsciente para o IM selecionado
    atualizar_inconsciente_para_im(memoria, dominio)

    ckpt = ckpt_path(dominio)
    if not os.path.exists(ckpt):
        st.warning("âš ï¸ Sem checkpoint â€” treine primeiro.")
        train(memoria, dominio)
        return

    (state,
     maxE, maxRE, maxCE, maxPIDE,
     mom_size, val_to_idx_E, val_to_idx_RE, val_to_idx_CE, val_to_idx_PIDE,
     vE, vRE, vCE, vPIDE,
     n_txt, n_emo, n_ctx,
     max_ng
     ) = torch.load(ckpt)

    model = AdamSegmentado(
        nE=len(vE), nRE=len(vRE),
        nCE=len(vCE), nPIDE=len(vPIDE),
        mom_size=mom_size,
        num_vals_E=len(val_to_idx_E), num_vals_RE=len(val_to_idx_RE),
        num_vals_CE=len(val_to_idx_CE), num_vals_PIDE=len(val_to_idx_PIDE),
        n_txt=n_txt, n_emo=n_emo,
        n_ctx=n_ctx,
        max_E=maxE, max_RE=maxRE, max_CE=maxCE, max_PIDE=maxPIDE, max_ng=max_ng
    )
    try:
        model.load_state_dict(state)
    except RuntimeError as e:
        st.warning(f"âš ï¸ Checkpoint incompatÃ­vel devido a mudanÃ§as na arquitetura: {e}. Retreinando...")
        train(memoria, dominio)
        return
    model.eval()

    blocos = memoria["IM"][dominio]["blocos"]
    inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
    ultimo_child_per_block = {}
    if dominio in inconsciente.get("INCO", {}):
        blocos_inco = inconsciente["INCO"][dominio].get("Blocos", [])
        for bloco in blocos_inco:
            bloco_num = int(bloco["Bloco_id"])
            saida_vals = [float(key) for key in bloco.get("SAÃDA", {}).keys()]
            if saida_vals:
                ultimo_child_per_block[bloco_num] = max(saida_vals)
            else:
                ultimo_child_per_block[bloco_num] = 0.50

    # Coletar todas as reaÃ§Ãµes possÃ­veis, incluindo variaÃ§Ãµes
    all_possible_reactions = set()
    for b in blocos:
        reac = b["entrada"].get("reacao", "")
        if reac:
            all_possible_reactions.add(reac)
        vars_reac = get_variations_for_tokens(dominio, b["bloco_id"], "Entrada", b["entrada"]["tokens"].get("RE", []))
        all_possible_reactions.update(vars_reac)

    # Mostrar nome do IM
    nome_im = memoria["IM"][dominio].get("nome", f"IM_{dominio}")
    st.write(f"**Conversando com: {nome_im}**")

    # Inicializar histÃ³rico de chat
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "variation" not in st.session_state:
        st.session_state.variation = 0
    if "current_bloco" not in st.session_state:
        st.session_state.current_bloco = None
    if "last_valid" not in st.session_state:
        st.session_state.last_valid = False

    # Exibir mensagens anteriores
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    def featurize(field: str, bloco: dict, max_len: int, vocab: dict, val_to_idx: dict, max_ng: int):
        tokens = bloco["entrada"]["tokens"].get(field, [])
        ngrams_list = [generate_ngrams(t, N_GRAM) for t in tokens]
        ids = [vocab.get(ng, vocab.get(UNK, 0)) for nglist in ngrams_list for ng in nglist]
        val_idxs = [val_to_idx.get(t, 0) for t in tokens]
        vals = [float(t) for t in tokens]
        moms = [int(t.split(".", 1)[0]) for t in tokens]
        if vals:
            min_v, max_v = min(vals), max(vals)
            pos = [(v - min_v) / (max_v - min_v) if max_v > min_v else 0.0 for v in vals]
        else:
            pos = []
        pad_ids = (max_len * max_ng) - len(ids)
        pad_vals = max_len - len(tokens)
        ids += [0] * pad_ids
        val_idxs += [0] * pad_vals
        vals += [0.0] * pad_vals
        moms += [0] * pad_vals
        pos += [0.0] * pad_vals
        return (
            torch.tensor([ids], dtype=torch.long),
            torch.tensor([val_idxs], dtype=torch.long),
            torch.tensor([vals], dtype=torch.float32),
            torch.tensor([moms], dtype=torch.long),
            torch.tensor([pos], dtype=torch.float32),
        )

    # Entrada do usuÃ¡rio
    st.info("ğŸ’¡ Para ver o Adam em aÃ§Ã£o, combine uma mensagem de texto com uma reaÃ§Ã£o (emoji) juntos! Exemplo: 'OlÃ¡ ğŸ˜Š'")
    if prompt := st.chat_input("Digite sua mensagem + reaÃ§Ã£o (ex: OlÃ¡ ğŸ˜Š)"):
        # Adicionar mensagem do usuÃ¡rio
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        cmd = prompt.lower().strip()

        if cmd == "sair":
            st.session_state.messages.append({"role": "assistant", "content": "ğŸ‘‹ AtÃ© mais!"})
            with st.chat_message("assistant"):
                st.markdown("ğŸ‘‹ AtÃ© mais!")
            st.session_state.messages = []
            st.session_state.variation = 0
            st.session_state.current_bloco = None
            return

        if cmd == "insight" and st.session_state.current_bloco:
            bloco = st.session_state.current_bloco
            ep_txt = bloco["entrada"]["texto"]
            ep_reac = bloco["entrada"].get("reacao", "")
            contexto = bloco["saidas"][0].get("contexto", "")
            emoji = bloco["saidas"][0].get("reacao", "")
            texts = bloco["saidas"][0]["textos"]
            chosen = texts[st.session_state.variation]
            insight_msg = f"ğŸ’¡ De acordo com a expressÃ£o â€œ{ep_txt}â€, a reaÃ§Ã£o â€œ{ep_reac}â€ e o contexto â€œ{contexto}â€, conclui que â€œ{chosen} {emoji}â€ Ã© a resposta mais adequada."
            st.session_state.messages.append({"role": "assistant", "content": insight_msg})
            with st.chat_message("assistant"):
                st.markdown(insight_msg)
            st.rerun()

        # Parse entrada normal
        txt, reac = parse_text_reaction(prompt, all_possible_reactions)
        bloco = None
        for b in blocos:
            txt_variations = get_variations_for_tokens(dominio, b["bloco_id"], "Entrada", b["entrada"]["tokens"]["E"])
            reac_variations = get_variations_for_tokens(dominio, b["bloco_id"], "Entrada", b["entrada"]["tokens"].get("RE", []))
            # Para reac, se RE tem tokens, mas reac Ã© o valor
            # Simplificar: comparar txt com variaÃ§Ãµes de E, reac com variaÃ§Ãµes de RE se houver
            # Mas reac Ã© string, talvez comparar diretamente se reac in reac_variations, mas reac_variations sÃ£o normalizados
            # Para reac, usar normalize(reac) in reac_variations
            # Mas reac_variations sÃ£o variaÃ§Ãµes dos tokens de RE
            # Se RE = ["ğŸ˜Š"], vars incluem outras reaÃ§Ãµes
            # EntÃ£o, if normalize(txt) in txt_variations and normalize(reac) in reac_variations:
            # Mas reac_variations sÃ£o variaÃ§Ãµes dos tokens de RE, que sÃ£o as reaÃ§Ãµes
            # Sim.
            if normalize(txt) in txt_variations and (not reac or normalize(reac) in reac_variations):
                bloco = b
                break
        if bloco is None:
            error_msg = "Desculpe mas seu texto e emoji nÃ£o existem neste universo. Por favor verifique sua mensagem e tente novamente."
            st.session_state.messages.append({"role": "assistant", "content": error_msg})
            with st.chat_message("assistant"):
                st.markdown(error_msg)
            st.session_state.last_valid = False
            st.rerun()

        st.session_state.current_bloco = bloco
        st.session_state.variation = 0
        st.session_state.last_valid = True

        # Preparo e forward
        max_val = ultimo_child_per_block.get(bloco["bloco_id"], 0.50)
        E_ids, E_val_idxs, E_val, E_mom, E_pos = featurize("E", bloco, maxE, vE, val_to_idx_E, max_ng)
        RE_ids, RE_val_idxs, RE_val, RE_mom, RE_pos = featurize("RE", bloco, maxRE, vRE, val_to_idx_RE, max_ng)
        CE_ids, CE_val_idxs, CE_val, CE_mom, CE_pos = featurize("CE", bloco, maxCE, vCE, val_to_idx_CE, max_ng)
        PI_ids, PI_val_idxs, PI_val, PI_mom, PI_pos = featurize("PIDE", bloco, maxPIDE, vPIDE, val_to_idx_PIDE, max_ng)

        x = {
            "E": E_ids, "E_val": E_val, "E_mom": E_mom, "E_pos": E_pos, "E_val_idx": E_val_idxs,
            "RE": RE_ids, "RE_val": RE_val, "RE_mom": RE_mom, "RE_pos": RE_pos, "RE_val_idx": RE_val_idxs,
            "CE": CE_ids, "CE_val": CE_val, "CE_mom": CE_mom, "CE_pos": CE_pos, "CE_val_idx": CE_val_idxs,
            "PIDE": PI_ids, "PIDE_val": PI_val, "PIDE_mom": PI_mom, "PIDE_pos": PI_pos, "PIDE_val_idx": PI_val_idxs,
        }

        with torch.no_grad():
            out = model(x)

        texts = bloco["saidas"][0]["textos"]
        emoji = bloco["saidas"][0].get("reacao", "")
        # Resposta randÃ´mica baseada nas saÃ­das do bloco (corpus prÃ³prio)
        import random
        chosen = random.choice(texts)
        response = f"{chosen} {emoji}"
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)
        st.rerun()

    # BotÃµes sempre visÃ­veis se hÃ¡ bloco atual e Ãºltima entrada foi vÃ¡lida
    if st.session_state.current_bloco and st.session_state.last_valid:
        col1, col2 = st.columns(2)
        with col1:
            if st.button("Enter"):
                texts = st.session_state.current_bloco["saidas"][0]["textos"]
                emoji = st.session_state.current_bloco["saidas"][0].get("reacao", "")
                st.session_state.variation = (st.session_state.variation + 1) % len(texts)
                chosen = texts[st.session_state.variation]
                response = f"{chosen} {emoji}"
                st.session_state.messages.append({"role": "assistant", "content": response})
                with st.chat_message("assistant"):
                    st.markdown(response)
                st.rerun()
        with col2:
            if st.button("ğŸ’¡ Insight"):
                bloco = st.session_state.current_bloco
                ep_txt = bloco["entrada"]["texto"]
                ep_reac = bloco["entrada"].get("reacao", "")
                contexto = bloco["saidas"][0].get("contexto", "")
                emoji = bloco["saidas"][0].get("reacao", "")
                texts = bloco["saidas"][0]["textos"]
                chosen = texts[st.session_state.variation]
                insight_msg = f"ğŸ’¡ De acordo com a expressÃ£o â€œ{ep_txt}â€, a reaÃ§Ã£o â€œ{ep_reac}â€ e o contexto â€œ{contexto}â€, conclui que â€œ{chosen} {emoji}â€ Ã© a resposta mais adequada."
                st.session_state.messages.append({"role": "assistant", "content": insight_msg})
                with st.chat_message("assistant"):
                    st.markdown(insight_msg)
                st.rerun()


def test_model(memoria: dict, dominio: str) -> None:
    # Atualizar inconsciente para o IM selecionado
    atualizar_inconsciente_para_im(memoria, dominio)

    ckpt = ckpt_path(dominio)
    if not os.path.exists(ckpt):
        st.warning("âš ï¸ Sem checkpoint â€” treine primeiro.");
        return

    (state,
     maxE, maxRE, maxCE, maxPIDE,
     mom_size, val_to_idx_E, val_to_idx_RE, val_to_idx_CE, val_to_idx_PIDE,
     vE, vRE, vCE, vPIDE,
     n_txt, n_emo, n_ctx,
     max_ng
     ) = torch.load(ckpt)

    model = AdamSegmentado(
        nE=len(vE), nRE=len(vRE), nCE=len(vCE), nPIDE=len(vPIDE),
        mom_size=mom_size,
        num_vals_E=len(val_to_idx_E), num_vals_RE=len(val_to_idx_RE),
        num_vals_CE=len(val_to_idx_CE), num_vals_PIDE=len(val_to_idx_PIDE),
        n_txt=n_txt, n_emo=n_emo,
        n_ctx=n_ctx,
        max_E=maxE, max_RE=maxRE, max_CE=maxCE, max_PIDE=maxPIDE, max_ng=max_ng
    )
    try:
        model.load_state_dict(state)
    except RuntimeError as e:
        st.warning(f"âš ï¸ Checkpoint incompatÃ­vel devido a mudanÃ§as na arquitetura: {e}. Treine primeiro.")
        return
    model.eval()

    blocos = memoria["IM"][dominio]["blocos"]
    inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
    ultimo_child_per_block = {}
    if dominio in inconsciente.get("INCO", {}):
        blocos_inco = inconsciente["INCO"][dominio].get("Blocos", [])
        for bloco in blocos_inco:
            bloco_num = int(bloco["Bloco_id"])
            saida_vals = [float(key) for key in bloco.get("SAÃDA", {}).keys()]
            if saida_vals:
                ultimo_child_per_block[bloco_num] = max(saida_vals)
            else:
                ultimo_child_per_block[bloco_num] = 0.50
    st.write(f"ğŸ“Š Teste em lote â€” DomÃ­nio {dominio} ({len(blocos)} blocos)")

    # Inicializar acumuladores para mÃ©tricas
    total_samples = 0
    acc_txt = 0.0
    acc_emo = 0.0
    acc_ctx = 0.0
    mse_pos = 0.0

    for b in blocos:
        max_val = ultimo_child_per_block.get(b["bloco_id"], 0.50)

        def featurize(field, max_len, vocab, val_to_idx, max_ng):
            tokens = b["entrada"]["tokens"].get(field, [])
            ngrams_list = [generate_ngrams(t, N_GRAM) for t in tokens]
            ids = [vocab.get(ng, vocab.get(UNK, 0)) for nglist in ngrams_list for ng in nglist]
            val_idxs = [val_to_idx.get(t, 0) for t in tokens]
            vals = [float(t) for t in tokens]
            moms = [int(t.split(".", 1)[0]) for t in tokens]
            if vals:
                min_v, max_v = min(vals), max(vals)
                pos = [(v - min_v) / (max_v - min_v) if max_v > min_v else 0.0 for v in vals]
            else:
                pos = []
            pad_ids = (max_len * max_ng) - len(ids)
            pad_vals = max_len - len(tokens)
            ids += [0] * pad_ids
            val_idxs += [0] * pad_vals
            vals += [0.0] * pad_vals
            moms += [0] * pad_vals
            pos += [0.0] * pad_vals
            return (
                torch.tensor([ids], dtype=torch.long),
                torch.tensor([val_idxs], dtype=torch.long),
                torch.tensor([vals], dtype=torch.float32),
                torch.tensor([moms], dtype=torch.long),
                torch.tensor([pos], dtype=torch.float32),
            )

        E_ids, E_val_idxs, E_val, E_mom, E_pos = featurize("E", maxE, vE, val_to_idx_E, max_ng)
        RE_ids, RE_val_idxs, RE_val, RE_mom, RE_pos = featurize("RE", maxRE, vRE, val_to_idx_RE, max_ng)
        CE_ids, CE_val_idxs, CE_val, CE_mom, CE_pos = featurize("CE", maxCE, vCE, val_to_idx_CE, max_ng)
        PI_ids, PI_val_idxs, PI_val, PI_mom, PI_pos = featurize("PIDE", maxPIDE, vPIDE, val_to_idx_PIDE, max_ng)

        x = {
            "E": E_ids, "E_val": E_val, "E_mom": E_mom, "E_pos": E_pos, "E_val_idx": E_val_idxs,
            "RE": RE_ids, "RE_val": RE_val, "RE_mom": RE_mom, "RE_pos": RE_pos, "RE_val_idx": RE_val_idxs,
            "CE": CE_ids, "CE_val": CE_val, "CE_mom": CE_mom, "CE_pos": CE_pos, "CE_val_idx": CE_val_idxs,
            "PIDE": PI_ids, "PIDE_val": PI_val, "PIDE_mom": PI_mom, "PIDE_pos": PI_pos, "PIDE_val_idx": PI_val_idxs,
        }

        with torch.no_grad():
            out = model(x)

        # Calcular mÃ©tricas para este bloco
        pred_txt = out["texto"].argmax(dim=1).item()
        pred_emo = out["emoji"].argmax(dim=1).item()
        pred_ctx = out["ctx"].argmax(dim=1).item()
        pred_pos = out["pos"].item()

        true_texts = [normalize(t) for t in b["saidas"][0]["textos"]]
        pred_text = b["saidas"][0]["textos"][pred_txt] if pred_txt < len(b["saidas"][0]["textos"]) else "N/A"
        true_emo = b["saidas"][0].get("reacao", "")
        true_ctx = b["saidas"][0].get("contexto", "")
        # Para pos, usar a mÃ©dia dos valores dos tokens do bloco
        all_vals = []
        for field in ["E", "RE", "CE", "PIDE"]:
            tokens = b["entrada"]["tokens"].get(field, [])
            all_vals.extend([float(t) for t in tokens])
        true_pos = sum(all_vals) / len(all_vals) if all_vals else 0.0

        # AcurÃ¡cias (comparar Ã­ndices)
        acc_txt_block = 1 if normalize(pred_text) in true_texts else 0
        acc_emo_block = 1 if pred_emo == 0 else 0  # 0 correto
        acc_ctx_block = 1 if pred_ctx == 0 else 0
        mse_pos_block = (pred_pos - true_pos) ** 2

        acc_txt += acc_txt_block
        acc_emo += acc_emo_block
        acc_ctx += acc_ctx_block
        mse_pos += mse_pos_block

        total_samples += 1

        # Coletar valores Ãºnicos no bloco
        block_vals = set()
        for field in ["E", "RE", "CE", "PIDE"]:
            block_vals.update(float(t) for t in b["entrada"]["tokens"].get(field, []) if t)

        st.write(f"\nâ Bloco_id={b['bloco_id']} Entrada: {b['entrada']['texto']} {b['entrada']['reacao']}")
        st.write(f"   Texto pred: {pred_text} | True: {true_texts}")
        st.write(f"   Emoji pred: {true_emo if pred_emo == 0 else 'Outro'} | True: {true_emo}")
        st.write(f"   Contexto pred: {true_ctx if pred_ctx == 0 else 'Outro'} | True: {true_ctx}")
        st.write(f"   PosiÃ§Ã£o pred: {pred_pos:.4f} | True: {true_pos:.4f}")
        st.write(f"   AcurÃ¡cia Texto: {acc_txt_block:.1f}")
        st.write(f"   AcurÃ¡cia Emoji: {acc_emo_block:.1f}")
        st.write(f"   AcurÃ¡cia Contexto: {acc_ctx_block:.1f}")
        st.write(f"   MSE PosiÃ§Ã£o: {mse_pos_block:.4f}")

    # Calcular mÃ©dias
    if total_samples > 0:
        acc_txt /= total_samples
        acc_emo /= total_samples
        acc_ctx /= total_samples
        mse_pos /= total_samples

        st.write("\nğŸ“ˆ MÃ©tricas Gerais:")
        st.write(f"AcurÃ¡cia Texto: {acc_txt:.2%}")
        st.write(f"AcurÃ¡cia Emoji: {acc_emo:.2%}")
        st.write(f"AcurÃ¡cia Contexto: {acc_ctx:.2%}")
        st.write(f"MSE PosiÃ§Ã£o: {mse_pos:.4f}")
    else:
        st.write("Nenhum bloco para testar.")


## INSEPA_CLI
def prompt_dominio(action: str, memoria: dict) -> str:
    """Lista IMs disponÃ­veis e permite escolher um para a aÃ§Ã£o especificada."""
    ims = list(memoria.get("IM", {}).keys())
    if not ims:
        st.error("âŒ Nenhum IM encontrado. Crie um primeiro.")
        return ""

    if action == "conversar":
        st.write("Selecione o Universo para conversar:")
    else:
        st.write(f"\n--- Escolher IM para {action} ---")
        st.write("IMs disponÃ­veis:")
        for im_id in ims:
            nome = memoria["IM"][im_id].get("nome", f"IM_{im_id}")
            num_blocos = len(memoria["IM"][im_id].get("blocos", []))
            st.write(f"- {im_id}: {nome} ({num_blocos} blocos)")

    dom = st.selectbox(f"Escolha o Universo para {action}", ims, key=f"dominio_{action}")
    return dom


def create_new_im(memoria: dict) -> None:
    """Cria um novo IM (Ãndice MÃ£e) vazio."""
    im_id = st.text_input("Ãndice mÃ£e para o novo IM:", key="new_im_id")
    if not im_id.isdigit():
        st.error("âŒ Ãndice mÃ£e deve ser um nÃºmero.")
        return
    im_id = int(im_id)
    if str(im_id) in memoria.get("IM", {}):
        st.error(f"âŒ IM {im_id} jÃ¡ existe.")
        return
    nome = st.text_input("Nome do IM (opcional):", key="new_im_name") or f"IM_{im_id}"
    if st.button("Criar IM"):
        memoria.setdefault("IM", {})[str(im_id)] = {
            "nome": nome,
            "ultimo_child": f"{im_id}.0",
            "blocos": []
        }
        salvar_json(ARQUIVO_MEMORIA, memoria)
        st.success(f"âœ… IM {im_id} criado: {nome}")


def submenu_im(memoria: dict) -> None:
    st.subheader("ğŸ› ï¸ Gerenciar IMs e Blocos")
    sub_opc = st.selectbox("Escolha uma opÃ§Ã£o:", [
        "ğŸ“‹ Visualizar IMs e Blocos",
        "â• Criar novo IM",
        "ğŸ”§ Gerar bloco a partir de template INSEPA",
        "ğŸ—‘ï¸ Apagar bloco",
        "ğŸš® Apagar IM",
        "âš™ï¸ Alimentar vars dos tokens",
        "â¬…ï¸ Voltar ao menu principal"
    ], key="submenu_im")

    if sub_opc == "ğŸ“‹ Visualizar IMs e Blocos":
        ims = list(memoria.get("IM", {}).keys())
        if not ims:
            st.info("Nenhum IM encontrado. Crie um primeiro.")
            return
        for im_id in ims:
            nome = memoria["IM"][im_id].get("nome", f"IM_{im_id}")
            num_blocos = len(memoria["IM"][im_id].get("blocos", []))
            with st.expander(f"ğŸ“ IM {im_id}: {nome} ({num_blocos} blocos)"):
                blocos = memoria["IM"][im_id].get("blocos", [])
                if blocos:
                    # Tabela de Entrada
                    data_entrada = [
                        {
                            "ID": b["bloco_id"],
                            "Entrada": b["entrada"]["texto"],
                            "ReaÃ§Ã£o": b["entrada"].get("reacao", ""),
                            "Contexto": b["entrada"].get("contexto", ""),
                            "Pensamento Interno": b["entrada"].get("pensamento_interno", "")
                        } for b in blocos
                    ]
                    st.subheader("ğŸ“¥ Entradas dos Blocos")
                    st.dataframe(data_entrada, use_container_width=True, column_config={
                        "Entrada": st.column_config.TextColumn("Entrada", width=None),
                        "ReaÃ§Ã£o": st.column_config.TextColumn("ReaÃ§Ã£o", width=None),
                        "Contexto": st.column_config.TextColumn("Contexto", width=None),
                        "Pensamento Interno": st.column_config.TextColumn("Pensamento Interno", width=None)
                    })
                    
                    # Tabela de SaÃ­da
                    data_saida = [
                        {
                            "ID": b["bloco_id"],
                            "SaÃ­das": "\n".join(b["saidas"][0]["textos"]),
                            "ReaÃ§Ã£o": b["saidas"][0].get("reacao", ""),
                            "Contexto": b["saidas"][0].get("contexto", "")
                        } for b in blocos
                    ]
                    st.subheader("ğŸ“¤ SaÃ­das dos Blocos")
                    st.dataframe(data_saida, use_container_width=True, column_config={
                        "SaÃ­das": st.column_config.TextColumn("SaÃ­das", width=None),
                        "ReaÃ§Ã£o": st.column_config.TextColumn("ReaÃ§Ã£o", width=None),
                        "Contexto": st.column_config.TextColumn("Contexto", width=None)
                    })
                    # Submenu para editar blocos
                    bloco_options = {f"ID {b['bloco_id']}: {b['entrada']['texto']}": b for b in blocos}
                    bloco_selecionado = st.selectbox("Selecione o bloco para editar:", list(bloco_options.keys()), key=f"edit_{im_id}")
                    bloco = bloco_options[bloco_selecionado]
                    with st.form(f"edit_bloco_{im_id}_{bloco['bloco_id']}"):
                        st.subheader("Editar Bloco")
                        entrada_texto = st.text_area("Entrada:", bloco["entrada"]["texto"], height=100)
                        entrada_reacao = st.text_input("ReaÃ§Ã£o (Entrada):", bloco["entrada"].get("reacao", ""))
                        entrada_contexto = st.text_area("Contexto (Entrada):", bloco["entrada"].get("contexto", ""), height=100)
                        entrada_pensamento = st.text_area("Pensamento Interno:", bloco["entrada"].get("pensamento_interno", ""), height=100)
                        saida_textos = st.text_area("SaÃ­da:", "\n".join(bloco["saidas"][0]["textos"]), height=150)
                        saida_reacao = st.text_input("ReaÃ§Ã£o (SaÃ­da):", bloco["saidas"][0].get("reacao", ""))
                        saida_contexto = st.text_area("Contexto (SaÃ­da):", bloco["saidas"][0].get("contexto", ""), height=100)
                        if st.form_submit_button("Salvar EdiÃ§Ãµes"):
                            bloco["entrada"]["texto"] = entrada_texto
                            bloco["entrada"]["reacao"] = entrada_reacao
                            bloco["entrada"]["contexto"] = entrada_contexto
                            bloco["entrada"]["pensamento_interno"] = entrada_pensamento
                            bloco["saidas"][0]["textos"] = saida_textos.split("\n")
                            bloco["saidas"][0]["reacao"] = saida_reacao
                            bloco["saidas"][0]["contexto"] = saida_contexto
                            recalcular_marcadores_im(memoria, im_id)
                            st.success("Bloco editado com sucesso!")
                else:
                    st.write("Nenhum bloco.")
    elif sub_opc == "â• Criar novo IM":
        create_new_im(memoria)
    elif sub_opc == "ğŸ”§ Gerar bloco a partir de template INSEPA":
        # Listar IMs existentes
        ims = list(memoria.get("IM", {}).keys())
        if not ims:
            st.error("âŒ Nenhum IM encontrado. Crie um primeiro.")
            return
        st.write("IMs disponÃ­veis:")
        for im_id in ims:
            nome = memoria["IM"][im_id].get("nome", f"IM_{im_id}")
            st.write(f"- {im_id}: {nome}")
        im_escolhido = st.selectbox("Digite o ID do IM:", ims, key="im_escolhido_gerar")
        st.write(f"Gerando blocos no IM {im_escolhido} (ou em outros se especificado no template)...")
        st.write("Cole seus blocos templates INSEPA separados por --- (cada um pode ter seu prÃ³prio 'Ãndice mÃ£e:' ou serÃ¡ usado o selecionado acima):")
        template_text = st.text_area("Templates:", key="template_text", height=300)
        if st.button("Gerar Blocos"):
            blocks = template_text.split("---")
            generated_count = 0
            for block in blocks:
                block = block.strip()
                if block:
                    if not block.startswith("Ãndice mÃ£e:"):
                        block = f"Ãndice mÃ£e: {im_escolhido}\n" + block
                    try:
                        generate_block_from_template(memoria, block)
                        generated_count += 1
                    except Exception as e:
                        st.error(f"âŒ Erro ao gerar bloco: {e}")
            if generated_count > 0:
                st.success(f"âœ… {generated_count} bloco(s) gerado(s) com sucesso!")
    elif sub_opc == "ğŸ—‘ï¸ Apagar bloco":
        # Apagar bloco
        ims = list(memoria.get("IM", {}).keys())
        if not ims:
            st.error("âŒ Nenhum IM encontrado.")
            return
        st.write("IMs disponÃ­veis:")
        for im_id in ims:
            nome = memoria["IM"][im_id].get("nome", f"IM_{im_id}")
            st.write(f"- {im_id}: {nome}")
        im_escolhido = st.selectbox("Digite o ID do IM:", ims, key="im_escolhido_apagar_bloco")
        universo = memoria["IM"][im_escolhido]
        blocos = universo.get("blocos", [])
        if not blocos:
            st.error("âŒ Nenhum bloco neste IM.")
            return
        st.write("Blocos no IM:")
        bloco_options = [f"ID {b['bloco_id']}: {b['entrada']['texto']}" for b in blocos]
        bloco_selecionado = st.selectbox("Selecione o bloco para apagar:", bloco_options, key="bloco_apagar")
        bid_apagar = bloco_selecionado.split(":")[0].split()[1]
        if st.button("Apagar Bloco"):
            try:
                bid_int = int(bid_apagar)
                bloco = next((b for b in blocos if b["bloco_id"] == bid_int), None)
                if bloco:
                    blocos.remove(bloco)
                    # Renumerar blocos sequencialmente
                    for i, b in enumerate(blocos, 1):
                        b["bloco_id"] = i
                    # Recalcular ultimo_child
                    if blocos:
                        universo["ultimo_child"] = max(b["saidas"][0]["fim"] for b in blocos)
                    else:
                        universo["ultimo_child"] = f"{im_escolhido}.0"
                    salvar_json(ARQUIVO_MEMORIA, memoria)

                    # Remover do inconsciente.json
                    inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
                    if im_escolhido in inconsciente.get("INCO", {}):
                        blocos_inco = inconsciente["INCO"][im_escolhido].get("Blocos", [])
                        inconsciente["INCO"][im_escolhido]["Blocos"] = [b for b in blocos_inco if b["Bloco_id"] != str(bid_int)]
                        # Recalcular Ultimo child se necessÃ¡rio
                        if blocos:
                            saida_vals = []
                            for b in blocos:
                                if im_escolhido in inconsciente.get("INCO", {}) and "Blocos" in inconsciente["INCO"][im_escolhido]:
                                    bloco_inco = next((bi for bi in inconsciente["INCO"][im_escolhido]["Blocos"] if bi["Bloco_id"] == str(b["bloco_id"])), None)
                                    if bloco_inco:
                                        saida_vals.extend(float(k) for k in bloco_inco.get("SAÃDA", {}).keys())
                            if saida_vals:
                                inconsciente["INCO"][im_escolhido]["Ultimo child"] = str(max(saida_vals))
                            else:
                                inconsciente["INCO"][im_escolhido]["Ultimo child"] = f"{im_escolhido}.0"
                        else:
                            if im_escolhido in inconsciente.get("INCO", {}):
                                inconsciente["INCO"][im_escolhido]["Ultimo child"] = f"{im_escolhido}.0"
                    salvar_json(ARQUIVO_INCONSCIENTE, inconsciente)
                    st.success(f"âœ… Bloco {bid_int} apagado. Blocos renumerados e ultimo_child ajustado.")
                else:
                    st.error("âŒ Bloco nÃ£o encontrado.")
            except ValueError:
                st.error("âŒ ID invÃ¡lido.")
    elif sub_opc == "ğŸš® Apagar IM":
        # Apagar IM
        ims = list(memoria.get("IM", {}).keys())
        if not ims:
            st.error("âŒ Nenhum IM encontrado.")
            return
        st.write("IMs disponÃ­veis:")
        for im_id in ims:
            nome = memoria["IM"][im_id].get("nome", f"IM_{im_id}")
            st.write(f"- {im_id}: {nome}")
        im_apagar = st.selectbox("Digite o ID do IM:", ims, key="im_apagar")
        confirm = st.checkbox("Tem certeza que quer apagar o IM e todos os seus blocos?")
        if confirm and st.button("Apagar IM"):
            # Coletar blocos para remover do inconsciente
            blocos_a_remover = [f"Bloco_{b['bloco_id']}" for b in memoria["IM"][im_apagar]["blocos"]]
            del memoria["IM"][im_apagar]
            salvar_json(ARQUIVO_MEMORIA, memoria)

            # Remover do inconsciente.json
            inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
            if im_apagar in inconsciente.get("INCO", {}):
                del inconsciente["INCO"][im_apagar]
            salvar_json(ARQUIVO_INCONSCIENTE, inconsciente)

            st.success(f"âœ… IM {im_apagar} apagado.")
    elif sub_opc == "âš™ï¸ Alimentar vars dos tokens":
        # Alimentar vars dos tokens
        ims = list(memoria.get("IM", {}).keys())
        if not ims:
            st.error("âŒ Nenhum IM encontrado.")
            return
        st.write("IMs disponÃ­veis:")
        for im_id in ims:
            nome = memoria["IM"][im_id].get("nome", f"IM_{im_id}")
            st.write(f"- {im_id}: {nome}")
        im_escolhido = st.selectbox("Digite o ID do IM:", ims, key="im_escolhido_vars")
        inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
        if im_escolhido not in inconsciente.get("INCO", {}):
            st.error("âŒ Nenhum bloco no inconsciente para este IM.")
            return
        im_data = inconsciente["INCO"][im_escolhido]
        blocos = im_data.get("Blocos", [])
        if not blocos:
            st.error("âŒ Nenhum bloco no inconsciente para este IM.")
            return
        st.write(f"Blocos do IM {im_escolhido}:")
        for bloco in blocos:
            with st.expander(f"Bloco {bloco['Bloco_id']}"):
                st.subheader("Entrada")
                for marker, data in bloco["Entrada"].items():
                    st.write(f"{marker}: {data['token']} | vars: {data['vars']}")
                st.subheader("SAÃDA")
                for marker, data in bloco["SAÃDA"].items():
                    st.write(f"{marker}: {data['token']} | vars: {data['vars']}")
        # Editar vars
        bloco_ids = [b["Bloco_id"] for b in blocos]
        bloco_edit = st.selectbox("Escolha o bloco para editar:", bloco_ids, key="bloco_edit")
        bloco = next((b for b in blocos if b["Bloco_id"] == bloco_edit), None)
        if bloco:
            campo_opc = st.selectbox("Escolha o campo:", ["Entrada", "SAÃDA"], key="campo_edit")
            campo = bloco[campo_opc]
            markers = list(campo.keys())
            marker_edit = st.selectbox("Escolha o marcador:", markers, key="marker_edit")
            current_vars = campo[marker_edit]["vars"]
            st.write(f"Vars atuais: {current_vars}")
            new_vars_str = st.text_input("Digite os novos vars separados por vÃ­rgula (ex: 0.1,0.2):", key="new_vars_edit")
            if st.button("Atualizar Vars"):
                new_vars = [v.strip() for v in new_vars_str.split(",") if v.strip()]
                if not new_vars:
                    st.error("âŒ Vars invÃ¡lidos.")
                    return
                campo[marker_edit]["vars"] = new_vars
                salvar_json(ARQUIVO_INCONSCIENTE, inconsciente)
                st.success(f"âœ… Vars atualizados para {marker_edit}: {new_vars}")

            # Gerar vars automaticamente com dicionÃ¡rio de sinÃ´nimos
            token = campo[marker_edit]["token"]
            word_to_search = new_vars_str.strip().split(',')[0].strip() if new_vars_str.strip() else token
            if st.button("Gerar Vars com DicionÃ¡rio", key="gerar_vars_dict"):
                try:
                    import re
                    import unidecode
                    st.write(f"Buscando sinÃ´nimos para a palavra: '{word_to_search}'")
                    clean_token = unidecode.unidecode(word_to_search.lower())
                    url = f"https://www.sinonimos.com.br/{clean_token}"
                    st.write(f"URL consultada: {url}")
                    content = ""
                    try:
                        # Tentar com requests primeiro
                        import requests
                        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                        response = requests.get(url, headers=headers)
                        st.write(f"Status da resposta (requests): {response.status_code}")
                        if response.status_code == 200:
                            content = response.text
                    except ImportError:
                        st.warning("Requests nÃ£o disponÃ­vel, tentando Selenium...")
                    
                    if not content:
                        # Fallback para Selenium
                        from selenium import webdriver
                        from selenium.webdriver.chrome.options import Options
                        options = Options()
                        options.add_argument("--headless")
                        options.add_argument("--no-sandbox")
                        options.add_argument("--disable-dev-shm-usage")
                        driver = webdriver.Chrome(options=options)
                        driver.get(url)
                        content = driver.page_source
                        driver.quit()
                        st.write("ConteÃºdo obtido via Selenium.")
                    
                    candidates = []
                    if content:
                        syn_links = re.findall(r'<a href="https://www\.sinonimos\.com\.br/[^"]+">([^<]+)</a>', content)
                        candidates = [s for s in syn_links if s.lower() != word_to_search.lower() and len(s) > 1][:5]
                    
                    if not candidates:
                        # Tentar com Selenium se requests nÃ£o encontrou
                        st.write("Tentando com Selenium...")
                        try:
                            from selenium import webdriver
                            from selenium.webdriver.chrome.options import Options
                            options = Options()
                            options.add_argument("--headless")
                            options.add_argument("--no-sandbox")
                            options.add_argument("--disable-dev-shm-usage")
                            driver = webdriver.Chrome(options=options)
                            driver.get(url)
                            content = driver.page_source
                            driver.quit()
                            st.write("ConteÃºdo obtido via Selenium.")
                            syn_links = re.findall(r'<a href="https://www\.sinonimos\.com\.br/[^"]+">([^<]+)</a>', content)
                            candidates = [s for s in syn_links if s.lower() != word_to_search.lower() and len(s) > 1][:5]
                        except Exception as e:
                            st.error(f"Erro com Selenium: {e}")
                    
                    if candidates:
                        st.write(f"SugestÃµes geradas: {candidates}")
                        selected = st.multiselect("Selecione as vars para adicionar:", candidates, key=f"select_{marker_edit}")
                        if st.button("Adicionar Selecionadas", key=f"add_{marker_edit}"):
                            current_vars = campo[marker_edit]["vars"]
                            new_vars = list(set(current_vars + selected))  # Evitar duplicatas
                            campo[marker_edit]["vars"] = new_vars
                            salvar_json(ARQUIVO_INCONSCIENTE, inconsciente)
                            st.success(f"âœ… Vars adicionadas: {selected}")
                    else:
                        st.warning("âš ï¸ Nenhuma variaÃ§Ã£o vÃ¡lida encontrada.")
                except ImportError as e:
                    if 'unidecode' in str(e):
                        st.error("âŒ Biblioteca 'unidecode' nÃ£o instalada. Instale com: pip install unidecode")
                    elif 'selenium' in str(e):
                        st.error("âŒ Biblioteca 'selenium' nÃ£o instalada. Instale com: pip install selenium")
                    else:
                        st.error(f"âŒ Erro de import: {e}")
                except Exception as e:
                    st.error(f"âŒ Erro ao buscar: {e}")
    elif sub_opc == "â¬…ï¸ Voltar ao menu principal":
        st.session_state.menu = "principal"


def recalcular_marcadores_im(memoria: dict, im_id: str) -> None:
    """Recalcula marcadores e tokens para todos os blocos do IM apÃ³s ediÃ§Ã£o."""
    universo = memoria["IM"][im_id]
    blocos = universo["blocos"]
    if not blocos:
        universo["ultimo_child"] = f"{im_id}.0"
        salvar_json(ARQUIVO_MEMORIA, memoria)
        return

    # Ordenar blocos por id
    blocos.sort(key=lambda b: b["bloco_id"])
    current_last = f"{im_id}.0"

    for bloco in blocos:
        # Retokenizar entrada
        E = Token(bloco["entrada"]["texto"])
        RE = [bloco["entrada"]["reacao"]] if bloco["entrada"]["reacao"] else []
        CE = Token(bloco["entrada"]["contexto"])
        pensamento_limpo = bloco["entrada"]["pensamento_interno"].strip('"')
        partes = pensamento_limpo.split('.')[:3]
        PIDE_full = []
        for parte in partes:
            PIDE_full.extend(Token(parte.strip()))
        PIDE_limited = PIDE_full[:3]

        # SaÃ­da
        S = []
        for t in bloco["saidas"][0]["textos"]:
            S += Token(t)
        RS = [bloco["saidas"][0]["reacao"]] if bloco["saidas"][0]["reacao"] else []
        CS = Token(bloco["saidas"][0]["contexto"])

        total_ent = len(E) + len(RE) + len(CE) + len(PIDE_limited)
        total_out = len(S) + len(RS) + len(CS)

        markers = generate_markers(current_last, total_ent + total_out)
        ent_marks = markers[:total_ent]
        out_marks = markers[total_ent:]

        fim_ent = ent_marks[-1] if ent_marks else current_last
        fim_out = out_marks[-1] if out_marks else fim_ent

        # Atualizar bloco
        idx = 0
        E_m = ent_marks[idx: idx + len(E)]; idx += len(E)
        RE_m = ent_marks[idx: idx + len(RE)]; idx += len(RE)
        CE_m = ent_marks[idx: idx + len(CE)]; idx += len(CE)
        PIDE_m = ent_marks[idx: idx + len(PIDE_limited)]

        jdx = 0
        S_m = out_marks[jdx: jdx + len(S)]; jdx += len(S)
        RS_m = out_marks[jdx: jdx + len(RS)]; jdx += len(RS)
        CS_m = out_marks[jdx: jdx + len(CS)]

        bloco["entrada"]["tokens"] = {
            "E": E_m,
            "RE": RE_m,
            "CE": CE_m,
            "PIDE": PIDE_m,
            "TOTAL": ent_marks
        }
        bloco["entrada"]["fim"] = fim_ent
        bloco["entrada"]["alnulu"] = len(bloco["entrada"]["texto"])

        bloco["saidas"][0]["tokens"] = {
            "S": S_m,
            "RS": RS_m,
            "CS": CS_m,
            "TOTAL": out_marks
        }
        bloco["saidas"][0]["fim"] = fim_out

        current_last = fim_out

    universo["ultimo_child"] = current_last
    salvar_json(ARQUIVO_MEMORIA, memoria)
    atualizar_inconsciente_para_im(memoria, im_id)


def atualizar_inconsciente_para_im(memoria: dict, im_id: str) -> None:
    """Atualiza o inconsciente.json com os dados do IM especificado."""
    if im_id not in memoria["IM"]:
        st.error(f"âŒ IM {im_id} nÃ£o encontrado.")
        return

    universo = memoria["IM"][im_id]
    blocos = universo.get("blocos", [])

    if not blocos:
        st.warning(f"âŒ IM {im_id} nÃ£o tem blocos.")
        return

    # Carregar inconsciente atual
    inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})

    # Preparar dados para o IM especÃ­fico
    im_data = {
        "NOME": universo["nome"],
        "Ultimo child": universo["ultimo_child"],
        "Blocos": []
    }

    for bloco in blocos:
        # Coletar todos os tokens da entrada
        E = Token(bloco["entrada"]["texto"])
        RE = [bloco["entrada"]["reacao"]] if bloco["entrada"]["reacao"] else []
        CE = Token(bloco["entrada"]["contexto"])
        pensamento_limpo = bloco["entrada"]["pensamento_interno"].strip('"')
        partes = pensamento_limpo.split('.')[:3]
        PIDE_full = []
        for parte in partes:
            PIDE_full.extend(Token(parte.strip()))
        entrada_tokens = E + RE + CE + PIDE_full

        # SaÃ­da
        S = []
        for t in bloco["saidas"][0]["textos"]:
            S += Token(t)
        RS = [bloco["saidas"][0]["reacao"]] if bloco["saidas"][0]["reacao"] else []
        CS = Token(bloco["saidas"][0]["contexto"])
        saida_tokens = S + RS + CS

        # Marcadores
        ent_marks = bloco["entrada"]["tokens"]["TOTAL"]
        if len(PIDE_full) > 3:
            extra_count = len(PIDE_full) - 3
            extra_marks = generate_markers(ent_marks[-1], extra_count)
            ent_marks_inco = ent_marks + extra_marks
        else:
            ent_marks_inco = ent_marks

        out_marks = bloco["saidas"][0]["tokens"]["TOTAL"]

        # Preservar vars existentes se o bloco jÃ¡ existir
        bloco_id_str = str(bloco["bloco_id"])
        existing_bloco = None
        if im_id in inconsciente.get("INCO", {}):
            existing_bloco = next((b for b in inconsciente["INCO"][im_id].get("Blocos", []) if b["Bloco_id"] == bloco_id_str), None)

        # Bloco data
        entrada_dict = {}
        for m, t in zip(ent_marks_inco, entrada_tokens):
            existing_vars = ["0.0"]
            if existing_bloco and m in existing_bloco.get("Entrada", {}):
                existing_vars = existing_bloco["Entrada"][m].get("vars", ["0.0"])
            entrada_dict[m] = {"token": t, "vars": existing_vars}

        saida_dict = {}
        for m, t in zip(out_marks, saida_tokens):
            existing_vars = ["0.0"]
            if existing_bloco and m in existing_bloco.get("SAÃDA", {}):
                existing_vars = existing_bloco["SAÃDA"][m].get("vars", ["0.0"])
            saida_dict[m] = {"token": t, "vars": existing_vars}

        bloco_data = {
            "Bloco_id": bloco_id_str,
            "Entrada": entrada_dict,
            "SAÃDA": saida_dict
        }
        im_data["Blocos"].append(bloco_data)

    # Atualizar apenas o IM especÃ­fico no inconsciente
    inconsciente.setdefault("INCO", {})[im_id] = im_data
    salvar_json(ARQUIVO_INCONSCIENTE, inconsciente)


def parse_template(lines: List[str]) -> Dict[str, Any]:
    tpl = {
        "indice_mae": None,
        "nome": "",
        "entrada": {"texto": "", "reacao": "", "contexto": "", "pensamento_interno": ""},
        "saida": {"textos": [], "reacao": "", "contexto": ""}
    }
    # quebra linhas que tiveram vÃ¡rios campos na mesma linha
    expanded: List[str] = []
    for raw in lines:
        tmp = _re.sub(
            r'(Ãndice mÃ£e:|Nome:|Entrada:|ReaÃ§Ã£o:|Contexto:|Pensamento Interno:|SaÃ­da:|\d+\.)',
            r'\n\1',
            raw
        )
        expanded += [l.strip() for l in tmp.split('\n') if l.strip()]

    section: Optional[str] = None
    for line in expanded:

        # Ãndice mÃ£e e Nome
        if line.startswith("Ãndice mÃ£e:"):
            tpl["indice_mae"] = int(line.split(":", 1)[1].strip())
            continue
        if line.startswith("Nome:"):
            tpl["nome"] = line.split(":", 1)[1].strip()
            continue

        # Entrada inline
        m_ent = _re.match(r'^Entrada:\s*(.+)', line)
        if m_ent:
            tpl["entrada"]["texto"] = m_ent.group(1).strip()
            section = "entrada"
            continue

        # SeÃ§Ãµes
        if line.startswith("Entrada:"):
            section = "entrada"
            continue
        if _re.match(r'^/?sa[Ã­i]da\s*:', line, _re.IGNORECASE):
            section = "saida_textos"
            continue

        # Campos de entrada
        if section == "entrada":
            if line.startswith("Texto:"):
                tpl["entrada"]["texto"] = line.split(":", 1)[1].strip()
            elif _re.match(r'^(reacao|reaÃ§Ã£o)\s*:', line, _re.IGNORECASE):
                tpl["entrada"]["reacao"] = line.split(":", 1)[1].strip()
            elif _re.match(r'^contexto\s*:', line, _re.IGNORECASE):
                tpl["entrada"]["contexto"] = line.split(":", 1)[1].strip()
            elif _re.match(r'^pensamento\s+interno\s*:', line, _re.IGNORECASE):
                tpl["entrada"]["pensamento_interno"] = line.split(":", 1)[1].strip()

        # Linhas de saÃ­da
        if section == "saida_textos":
            m = _re.match(r'^\d+\.\s*(.+)', line)
            if m:
                tpl["saida"]["textos"].append(m.group(1).strip())
            elif line.strip() and not _re.match(r'^(reacao|reaÃ§Ã£o|contexto)\s*:', line, _re.IGNORECASE):
                # Continuar o Ãºltimo texto se nÃ£o Ã© um novo campo
                if tpl["saida"]["textos"]:
                    tpl["saida"]["textos"][-1] += " " + line.strip()
            else:
                section = "saida_meta"

        # Campos de meta-saÃ­da (parse sempre se section == "saida_meta")
        if section == "saida_meta":
            if _re.match(r'^(reacao|reaÃ§Ã£o)\s*:', line, _re.IGNORECASE):
                tpl["saida"]["reacao"] = line.split(":", 1)[1].strip()
            elif _re.match(r'^contexto\s*:', line, _re.IGNORECASE):
                tpl["saida"]["contexto"] = line.split(":", 1)[1].strip()

    if tpl["indice_mae"] is None:
        raise ValueError("'Ãndice mÃ£e' nÃ£o encontrado no template.")
    return tpl


def generate_block_from_template(memoria: dict, template_text: str) -> None:
    """Gera bloco INSEPA a partir de template colado e adiciona ao adam_memoria.json."""
    lines = template_text.splitlines()
    tpl = parse_template(lines)
    mom = str(tpl["indice_mae"])
    universo = memoria["IM"].get(mom)

    # Cria IM se nÃ£o existir
    if universo is None:
        universo = {
            "nome": tpl["nome"] or f"IM_{mom}",
            "ultimo_child": f"{mom}.0",
            "blocos": []
        }
        memoria["IM"][mom] = universo

    last = universo["ultimo_child"]

    # TokenizaÃ§Ã£o
    E = Token(tpl["entrada"]["texto"])
    RE = [tpl["entrada"]["reacao"]] if tpl["entrada"]["reacao"] else []
    CE = Token(tpl["entrada"]["contexto"])
    # Limpar e dividir pensamento_interno
    pensamento_limpo = tpl["entrada"]["pensamento_interno"].strip('"')
    tpl["entrada"]["pensamento_interno"] = pensamento_limpo  # Salvar limpo no adam_memoria
    partes = pensamento_limpo.split('.')[:3]  # Dividir em atÃ© 3 sequÃªncias por '.' 
    PIDE_full = []
    for parte in partes:
        PIDE_full.extend(Token(parte.strip()))
    PIDE_limited = PIDE_full[:3]

    S: List[str] = []
    for t in tpl["saida"]["textos"]:
        S += Token(t)
    RS = [tpl["saida"]["reacao"]] if tpl["saida"]["reacao"] else []
    CS = Token(tpl["saida"]["contexto"])

    total_ent = len(E) + len(RE) + len(CE) + len(PIDE_limited)
    total_out = len(S) + len(RS) + len(CS)

    markers = generate_markers(last, total_ent + total_out)
    ent_marks = markers[:total_ent]
    out_marks = markers[total_ent:]

    fim_ent = ent_marks[-1] if ent_marks else last
    fim_out = out_marks[-1] if out_marks else fim_ent

    # Subdivide
    idx = 0
    E_m = ent_marks[idx: idx + len(E)];
    idx += len(E)
    RE_m = ent_marks[idx: idx + len(RE)];
    idx += len(RE)
    CE_m = ent_marks[idx: idx + len(CE)];
    idx += len(CE)
    PIDE_m = ent_marks[idx: idx + len(PIDE_limited)]

    jdx = 0
    S_m = out_marks[jdx: jdx + len(S)];
    jdx += len(S)
    RS_m = out_marks[jdx: jdx + len(RS)];
    jdx += len(RS)
    CS_m = out_marks[jdx: jdx + len(CS)]

    next_id = max((b["bloco_id"] for b in universo["blocos"]), default=0) + 1

    new_block: Dict[str, Any] = {
        "bloco_id": next_id,
        "entrada": {
            **tpl["entrada"],
            "tokens": {
                "E": E_m,
                "RE": RE_m,
                "CE": CE_m,
                "PIDE": PIDE_m,
                "TOTAL": ent_marks
            },
            "fim": fim_ent,
            "alnulu": len(tpl["entrada"]["texto"])
        },
        "saidas": [{
            **tpl["saida"],
            "tokens": {
                "S": S_m,
                "RS": RS_m,
                "CS": CS_m,
                "TOTAL": out_marks
            },
            "fim": fim_out
        }],
        "open": True
    }

    universo["blocos"].append(new_block)
    universo["ultimo_child"] = fim_out

    salvar_json(ARQUIVO_MEMORIA, memoria)

    # Atualizar inconsciente.json com o novo bloco
    inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"INCO": {}})
    all_ent_tokens = E + RE + CE + PIDE_full
    all_out_tokens = S + RS + CS
    ent_marks_inco = ent_marks[:]
    if len(PIDE_full) > 3:
        extra_count = len(PIDE_full) - 3
        extra_marks = generate_markers(ent_marks[-1], extra_count)
        ent_marks_inco.extend(extra_marks)

    # Bloco data
    bloco_data = {
        "Bloco_id": str(next_id),
        "Entrada": {m: {"token": t, "vars": ["0.0"]} for m, t in zip(ent_marks_inco, all_ent_tokens)},
        "SAÃDA": {m: {"token": t, "vars": ["0.0"]} for m, t in zip(out_marks, all_out_tokens)}
    }

    # Preparar dados para o IM especÃ­fico
    if mom in inconsciente.get("INCO", {}):
        im_data = inconsciente["INCO"][mom]
        im_data["Blocos"].append(bloco_data)
        im_data["Ultimo child"] = fim_out
    else:
        im_data = {
            "NOME": universo["nome"],
            "Ultimo child": fim_out,
            "Blocos": [bloco_data]
        }

    # Atualizar apenas o IM especÃ­fico no inconsciente
    inconsciente.setdefault("INCO", {})[mom] = im_data
    salvar_json(ARQUIVO_INCONSCIENTE, inconsciente)

    st.write(f"âœ… Bloco adicionado ao domÃ­nio {mom}. Ãšltimo marker: {fim_out}")


def submenu_estatisticas(memoria: dict) -> None:
    st.subheader("ğŸ“Š EstatÃ­sticas do Sistema INSEPA")
    
    # NÃºmero de IMs
    num_ims = len(memoria.get("IM", {}))
    st.metric("NÃºmero de IMs", num_ims)
    
    if num_ims > 0:
        # Dados para grÃ¡ficos
        im_names = []
        num_blocos = []
        total_blocos = 0
        for im_id, im_data in memoria["IM"].items():
            nome = im_data.get("nome", f"IM_{im_id}")
            blocos = len(im_data.get("blocos", []))
            im_names.append(nome)
            num_blocos.append(blocos)
            total_blocos += blocos
        
        st.metric("Total de Blocos", total_blocos)
        
        # GrÃ¡fico de barras: Blocos por IM
        import pandas as pd
        df_blocos = pd.DataFrame({"IM": im_names, "Blocos": num_blocos})
        st.bar_chart(df_blocos.set_index("IM"))
        
        # EstatÃ­sticas adicionais
        if total_blocos > 0:
            avg_blocos = total_blocos / num_ims
            st.metric("MÃ©dia de Blocos por IM", f"{avg_blocos:.1f}")
            
            # DistribuiÃ§Ã£o de reaÃ§Ãµes
            reacoes = {}
            for im_data in memoria["IM"].values():
                for bloco in im_data.get("blocos", []):
                    reac = bloco["entrada"].get("reacao", "")
                    if reac:
                        reacoes[reac] = reacoes.get(reac, 0) + 1
            
            if reacoes:
                df_reacoes = pd.DataFrame(list(reacoes.items()), columns=["ReaÃ§Ã£o", "Contagem"])
                st.subheader("DistribuiÃ§Ã£o de ReaÃ§Ãµes de Entrada")
                st.bar_chart(df_reacoes.set_index("ReaÃ§Ã£o"))
        
        # Verificar se hÃ¡ checkpoints treinados
        import os
        ckpts = [f for f in os.listdir(".") if f.startswith("insepa_") and f.endswith(".pt")]
        st.metric("Modelos Treinados", len(ckpts))
        if ckpts:
            st.write("Modelos disponÃ­veis:")
            for ckpt in ckpts:
                dom = ckpt.replace("insepa_", "").replace(".pt", "")
                st.write(f"- {dom}")
    else:
        st.info("Nenhum IM criado ainda.")


def main():
    st.set_page_config(layout="wide")
    # CSS harmonizado: fundo roxo, menu preto com tÃ­tulos brancos
    st.markdown("""
    <style>
    .stApp {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 300 50'%3E%3Ctext fill='rgba(255,255,255,0.1)' font-size='20' x='150' y='25' text-anchor='middle'%3EğŸ¤– Adam ğŸ˜Š Amor ğŸ’œ INSEPA ğŸŒŸ%3C/text%3E%3C/svg%3E");
        background-repeat: repeat;
        background-size: 300px 50px;
    }
    .stButton>button {
        background: black;
        color: white;
        border: 1px solid white;
        border-radius: 5px;
        padding: 8px 16px;
    }
    .stButton>button:hover {
        background: #333;
        color: white;
    }
    .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {
        color: white;
    }
    .stTextInput input, .stSelectbox select, .stTextArea textarea {
        background: rgba(255, 255, 255, 0.1);
        color: white;
        border: 1px solid white;
    }
    .stDataFrame {
        background: rgba(255, 255, 255, 0.1);
    }
    .stSidebar {
        background: rgba(0, 0, 0, 0.8);
        backdrop-filter: blur(10px);
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.title("ğŸ¤– Adam Lovely AI - Sistema INSEPA")
    st.markdown("### Interface de Chat com IA AvanÃ§ada")
    memoria = carregar_json(ARQUIVO_MEMORIA, {"IM": {}})
    inconsciente = carregar_json(ARQUIVO_INCONSCIENTE, {"conteudos": []})

    # Menu no canto esquerdo
    with st.sidebar:
        st.header("Menu")
        if st.button("ğŸ—ï¸ Gerenciar IMs"):
            st.session_state.menu = "gerenciar"
        if st.button("ğŸ§  Treinar"):
            st.session_state.menu = "treinar"
        if st.button("ğŸ§ª Testar"):
            st.session_state.menu = "testar"
        if st.button("ğŸ’¬ Conversar"):
            st.session_state.menu = "conversar"
        if st.button("ğŸ“Š EstatÃ­sticas"):
            st.session_state.menu = "estatisticas"
        if st.button("âŒ Sair"):
            st.write("ğŸ‘‹ AtÃ© mais!")
            st.stop()

    if "menu" not in st.session_state:
        st.session_state.menu = "conversar"

    if st.session_state.menu == "gerenciar":
        submenu_im(memoria)
    elif st.session_state.menu == "treinar":
        dom = prompt_dominio("treinar", memoria)
        if dom:
            if dom in memoria["IM"]:
                train(memoria, dom)
            else:
                st.error(f"âŒ DomÃ­nio '{dom}' nÃ£o encontrado.")
    elif st.session_state.menu == "testar":
        dom = prompt_dominio("testar", memoria)
        if dom:
            if dom in memoria["IM"]:
                test_model(memoria, dom)
            else:
                st.error(f"âŒ DomÃ­nio '{dom}' nÃ£o encontrado.")
    elif st.session_state.menu == "conversar":
        dom = prompt_dominio("conversar", memoria)
        if dom:
            if dom in memoria["IM"]:
                infer(memoria, dom)
            else:
                st.error(f"âŒ DomÃ­nio '{dom}' nÃ£o encontrado.")
    elif st.session_state.menu == "estatisticas":
        submenu_estatisticas(memoria)


if __name__ == "__main__":
    main()
